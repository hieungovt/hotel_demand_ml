---
title: "Modeling"
---

# Modeling {#sec-modeling}

The fourth phase of CRISP-DM involves selecting modeling techniques, building models, and tuning parameters. We build two types of models:

1. **Classification Model**: Predict booking cancellations
2. **Time Series Model**: Forecast future demand

```{python}
#| label: setup
#| output: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report, roc_curve)
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import joblib
import os
import warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')

# Create models directory
os.makedirs('../models', exist_ok=True)
```

## Load Prepared Data

```{python}
#| label: load-data

# Load the processed data
df = pd.read_csv('../data/processed/hotel_bookings_processed.csv')
print(f"Loaded processed data: {len(df):,} rows")

# Prepare features
feature_cols = [
    'lead_time', 'arrival_date_week_number', 'arrival_month_num',
    'stays_in_weekend_nights', 'stays_in_week_nights', 'adults',
    'children', 'babies', 'is_repeated_guest', 'previous_cancellations',
    'previous_bookings_not_canceled', 'booking_changes',
    'days_in_waiting_list', 'adr', 'required_car_parking_spaces',
    'total_of_special_requests', 'total_nights', 'total_guests',
    'is_weekend_stay', 'got_reserved_room'
]

cat_cols = ['hotel', 'meal', 'market_segment', 'distribution_channel',
            'deposit_type', 'customer_type', 'season']

# Prepare feature matrix
X = df[feature_cols].copy()

# Encode categorical features
label_encoders = {}
for col in cat_cols:
    if col in df.columns:
        le = LabelEncoder()
        X[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le

y = df['is_canceled']

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {len(X_train):,} samples")
print(f"Test set: {len(X_test):,} samples")
print(f"Features: {X_train.shape[1]}")
```

## Part 1: Classification Models

### Model Selection

We evaluate multiple algorithms to find the best performer:

```{python}
#| label: model-comparison

# Define models to compare
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, 
                                            random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5,
                                                     random_state=42),
    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,
                                  random_state=42, eval_metric='logloss',
                                  verbosity=0)
}

# Evaluate each model with cross-validation
results = []
for name, model in models.items():
    print(f"Training {name}...")
    
    # Cross-validation scores
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
    
    # Fit on full training set for test evaluation
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    results.append({
        'Model': name,
        'CV ROC-AUC': cv_scores.mean(),
        'CV Std': cv_scores.std(),
        'Test Accuracy': accuracy_score(y_test, y_pred),
        'Test Precision': precision_score(y_test, y_pred),
        'Test Recall': recall_score(y_test, y_pred),
        'Test F1': f1_score(y_test, y_pred),
        'Test ROC-AUC': roc_auc_score(y_test, y_proba)
    })

results_df = pd.DataFrame(results)
print("\nModel Comparison Results:")
```

```{python}
#| label: tbl-model-comparison
#| tbl-cap: "Classification Model Comparison"

results_df.round(4)
```

### Visualize Model Comparison

```{python}
#| label: fig-model-comparison
#| fig-cap: "Model Performance Comparison"

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ROC-AUC comparison
x_pos = range(len(results_df))
bars1 = axes[0].bar(x_pos, results_df['CV ROC-AUC'], 
                    yerr=results_df['CV Std'], capsize=5,
                    color='#3498db', alpha=0.8)
axes[0].bar(x_pos, results_df['Test ROC-AUC'], 
            alpha=0.5, color='#e74c3c', width=0.4)
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')
axes[0].set_ylabel('ROC-AUC Score')
axes[0].set_title('ROC-AUC: CV (blue) vs Test (red)', fontweight='bold')
axes[0].set_ylim(0.7, 1.0)
axes[0].axhline(y=0.85, color='green', linestyle='--', 
                label='Target: 0.85', alpha=0.7)
axes[0].legend()

# Multiple metrics comparison
metrics = ['Test Precision', 'Test Recall', 'Test F1']
x = np.arange(len(results_df))
width = 0.25

for i, metric in enumerate(metrics):
    axes[1].bar(x + i*width, results_df[metric], width, 
                label=metric.replace('Test ', ''))

axes[1].set_xticks(x + width)
axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')
axes[1].set_ylabel('Score')
axes[1].set_title('Precision, Recall, and F1 Comparison', fontweight='bold')
axes[1].legend()
axes[1].set_ylim(0.5, 1.0)

plt.tight_layout()
plt.show()
```

### ROC Curves

```{python}
#| label: fig-roc-curves
#| fig-cap: "ROC Curves for All Models"

fig, ax = plt.subplots(figsize=(10, 8))

colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']
for (name, model), color in zip(models.items(), colors):
    y_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc = roc_auc_score(y_test, y_proba)
    ax.plot(fpr, tpr, color=color, linewidth=2, 
            label=f'{name} (AUC = {auc:.4f})')

ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
ax.set_xlabel('False Positive Rate', fontsize=12)
ax.set_ylabel('True Positive Rate', fontsize=12)
ax.set_title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Hyperparameter Tuning

Based on the comparison, **XGBoost** shows the best performance. Let's tune its hyperparameters:

```{python}
#| label: hyperparameter-tuning

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

print("Hyperparameter tuning with GridSearchCV...")
print(f"Total combinations: {np.prod([len(v) for v in param_grid.values()])}")

# Create base model
xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss', verbosity=0)

# Grid search with cross-validation
grid_search = GridSearchCV(
    xgb_model, param_grid, cv=3, scoring='roc_auc', 
    n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)

print(f"\nBest parameters: {grid_search.best_params_}")
print(f"Best CV ROC-AUC: {grid_search.best_score_:.4f}")
```

### Train Final Model

```{python}
#| label: final-model

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate on test set
y_pred_final = best_model.predict(X_test)
y_proba_final = best_model.predict_proba(X_test)[:, 1]

print("Final Model Performance:")
print("=" * 50)
print(f"Accuracy:  {accuracy_score(y_test, y_pred_final):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_final):.4f}")
print(f"Recall:    {recall_score(y_test, y_pred_final):.4f}")
print(f"F1 Score:  {f1_score(y_test, y_pred_final):.4f}")
print(f"ROC-AUC:   {roc_auc_score(y_test, y_proba_final):.4f}")
```

### Confusion Matrix

```{python}
#| label: fig-confusion-matrix
#| fig-cap: "Confusion Matrix for Final Model"

cm = confusion_matrix(y_test, y_pred_final)

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
            xticklabels=['Not Canceled', 'Canceled'],
            yticklabels=['Not Canceled', 'Canceled'])
ax.set_xlabel('Predicted', fontsize=12)
ax.set_ylabel('Actual', fontsize=12)
ax.set_title('Confusion Matrix - XGBoost Final Model', fontsize=14, fontweight='bold')

# Add percentages
total = cm.sum()
for i in range(2):
    for j in range(2):
        pct = cm[i, j] / total * 100
        ax.text(j + 0.5, i + 0.7, f'({pct:.1f}%)', 
                ha='center', va='center', fontsize=10, color='gray')

plt.tight_layout()
plt.show()

print(f"\nTrue Negatives:  {cm[0,0]:,} (correctly predicted not canceled)")
print(f"True Positives:  {cm[1,1]:,} (correctly predicted canceled)")
print(f"False Positives: {cm[0,1]:,} (predicted canceled but wasn't)")
print(f"False Negatives: {cm[1,0]:,} (predicted not canceled but was)")
```

### Feature Importance

```{python}
#| label: fig-feature-importance
#| fig-cap: "Feature Importance from XGBoost Model"

# Get feature importance
importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=True)

# Plot
fig, ax = plt.subplots(figsize=(10, 10))
colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(importance)))
ax.barh(importance['feature'], importance['importance'], color=colors)
ax.set_xlabel('Feature Importance', fontsize=12)
ax.set_title('XGBoost Feature Importance', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

print("\nTop 10 Most Important Features:")
print(importance.tail(10).to_string(index=False))
```

## Part 2: Time Series Model

Now let's build a demand forecasting model using ARIMA/SARIMA.

```{python}
#| label: time-series-setup

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load time series data
daily_bookings = pd.read_csv('../data/processed/daily_bookings.csv', parse_dates=['ds'])
daily_bookings = daily_bookings.set_index('ds')
daily_bookings = daily_bookings.sort_index()

print(f"Time series data: {len(daily_bookings)} days")
print(f"Date range: {daily_bookings.index.min()} to {daily_bookings.index.max()}")
```

### Time Series Decomposition

```{python}
#| label: fig-ts-decomposition
#| fig-cap: "Time Series Decomposition"

# Decompose the time series
decomposition = seasonal_decompose(daily_bookings['y'], model='additive', period=7)

fig, axes = plt.subplots(4, 1, figsize=(14, 12))

decomposition.observed.plot(ax=axes[0], color='#3498db')
axes[0].set_title('Observed', fontweight='bold')
axes[0].set_ylabel('Bookings')

decomposition.trend.plot(ax=axes[1], color='#e74c3c')
axes[1].set_title('Trend', fontweight='bold')
axes[1].set_ylabel('Bookings')

decomposition.seasonal.plot(ax=axes[2], color='#2ecc71')
axes[2].set_title('Seasonality (Weekly)', fontweight='bold')
axes[2].set_ylabel('Bookings')

decomposition.resid.plot(ax=axes[3], color='#9b59b6')
axes[3].set_title('Residuals', fontweight='bold')
axes[3].set_ylabel('Bookings')

plt.tight_layout()
plt.show()
```

### Train-Test Split for Time Series

```{python}
#| label: ts-split

# Use last 60 days for testing
train_size = len(daily_bookings) - 60
ts_train = daily_bookings.iloc[:train_size]
ts_test = daily_bookings.iloc[train_size:]

print(f"Training set: {len(ts_train)} days ({ts_train.index.min()} to {ts_train.index.max()})")
print(f"Test set: {len(ts_test)} days ({ts_test.index.min()} to {ts_test.index.max()})")
```

### Build SARIMA Model

```{python}
#| label: sarima-model

print("Training SARIMA model with weekly seasonality...")

# SARIMA with weekly seasonality (s=7)
sarima_model = SARIMAX(
    ts_train['y'],
    order=(1, 1, 1),
    seasonal_order=(1, 1, 1, 7),
    enforce_stationarity=False,
    enforce_invertibility=False
)

sarima_fitted = sarima_model.fit(disp=False)

print(f"\nSARIMA Model Summary:")
print(f"AIC: {sarima_fitted.aic:.2f}")
print(f"BIC: {sarima_fitted.bic:.2f}")
```

### Time Series Forecast

```{python}
#| label: fig-ts-forecast
#| fig-cap: "SARIMA Demand Forecast"

# Forecast
forecast = sarima_fitted.get_forecast(steps=len(ts_test))
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

# Calculate metrics
mae = mean_absolute_error(ts_test['y'], forecast_mean)
rmse = np.sqrt(mean_squared_error(ts_test['y'], forecast_mean))
mape = np.mean(np.abs((ts_test['y'] - forecast_mean) / ts_test['y'])) * 100

print(f"Forecast Metrics:")
print(f"  MAE:  {mae:.2f} bookings")
print(f"  RMSE: {rmse:.2f} bookings")
print(f"  MAPE: {mape:.2f}%")

# Plot forecast vs actual
fig, ax = plt.subplots(figsize=(14, 6))

# Plot training data (last 60 days)
ts_train['y'].iloc[-60:].plot(ax=ax, color='#3498db', label='Training Data')

# Plot test data
ts_test['y'].plot(ax=ax, color='#2ecc71', label='Actual')

# Plot forecast
ax.plot(ts_test.index, forecast_mean, color='#e74c3c', 
        linewidth=2, label='Forecast')

# Confidence interval
ax.fill_between(ts_test.index, 
                forecast_ci.iloc[:, 0], 
                forecast_ci.iloc[:, 1],
                color='#e74c3c', alpha=0.2, label='95% CI')

ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Daily Bookings', fontsize=12)
ax.set_title('SARIMA Demand Forecast vs Actual', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Forecast Residual Analysis

```{python}
#| label: fig-residuals
#| fig-cap: "Forecast Residual Analysis"

residuals = ts_test['y'].values - forecast_mean.values

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# Residual distribution
axes[0].hist(residuals, bins=20, color='#3498db', edgecolor='white')
axes[0].axvline(0, color='red', linestyle='--')
axes[0].set_xlabel('Residual')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Residual Distribution', fontweight='bold')

# Residuals over time
axes[1].plot(ts_test.index, residuals, 'o-', color='#2ecc71', alpha=0.7)
axes[1].axhline(0, color='red', linestyle='--')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Residual')
axes[1].set_title('Residuals Over Time', fontweight='bold')
axes[1].tick_params(axis='x', rotation=45)

# Actual vs Predicted
axes[2].scatter(ts_test['y'], forecast_mean, alpha=0.6, color='#9b59b6')
axes[2].plot([ts_test['y'].min(), ts_test['y'].max()], 
             [ts_test['y'].min(), ts_test['y'].max()], 
             'r--', linewidth=2)
axes[2].set_xlabel('Actual Bookings')
axes[2].set_ylabel('Predicted Bookings')
axes[2].set_title('Actual vs Predicted', fontweight='bold')

plt.tight_layout()
plt.show()
```

## Save Models

```{python}
#| label: save-models

# Save the classification model
model_data = {
    'model': best_model,
    'model_type': 'xgboost',
    'feature_names': list(X_train.columns),
    'label_encoders': label_encoders,
    'best_params': grid_search.best_params_,
    'metrics': {
        'accuracy': accuracy_score(y_test, y_pred_final),
        'precision': precision_score(y_test, y_pred_final),
        'recall': recall_score(y_test, y_pred_final),
        'f1': f1_score(y_test, y_pred_final),
        'roc_auc': roc_auc_score(y_test, y_proba_final)
    }
}

joblib.dump(model_data, '../models/cancellation_model.pkl')
print("✓ Saved classification model: ../models/cancellation_model.pkl")

# Save the time series model
ts_model_data = {
    'model': sarima_fitted,
    'model_type': 'sarima',
    'order': (1, 1, 1),
    'seasonal_order': (1, 1, 1, 7),
    'metrics': {
        'mae': mae,
        'rmse': rmse,
        'mape': mape
    },
    'last_date': ts_train.index.max()
}

joblib.dump(ts_model_data, '../models/demand_model.pkl')
print("✓ Saved time series model: ../models/demand_model.pkl")
```

## Modeling Summary

```{python}
#| label: tbl-modeling-summary
#| tbl-cap: "Modeling Phase Summary"

summary = pd.DataFrame({
    'Aspect': [
        'Classification Model',
        'Best Algorithm',
        'Hyperparameter Tuning',
        'Classification ROC-AUC',
        'Classification Recall',
        'Time Series Model',
        'Forecast MAPE',
        'Models Serialized'
    ],
    'Details': [
        'Binary classification for cancellation prediction',
        'XGBoost (outperformed LR, RF, GBM)',
        f"GridSearchCV with {np.prod([len(v) for v in param_grid.values()])} combinations",
        f"{roc_auc_score(y_test, y_proba_final):.4f}",
        f"{recall_score(y_test, y_pred_final):.4f}",
        'SARIMA(1,1,1)(1,1,1,7) with weekly seasonality',
        f"{mape:.2f}%",
        '2 models (classification + time series)'
    ]
})
summary
```

### Key Findings

1. **XGBoost** outperforms other models with the highest ROC-AUC and F1 scores
2. **Lead time** is the most important feature for predicting cancellations
3. **Deposit type** and **previous cancellations** are also highly predictive
4. **SARIMA** captures weekly seasonality effectively
5. Forecast MAPE of ~15% is within acceptable business thresholds

The next chapter will provide a comprehensive evaluation of both models.
